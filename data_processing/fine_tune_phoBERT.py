import pandas as pd
from datasets import Dataset, load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
import numpy as np
import evaluate
import os

# ----------------------------------------------------------------------
# 1. CHU·∫®N B·ªä D·ªÆ LI·ªÜU - ƒê·ªåC T·ª™ FILE CSV SUPPORT SET
# ----------------------------------------------------------------------

# ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV support_set.csv (4-shot cho m·ªói emotional tag)
support_set_path = "support_set.csv"

if not os.path.exists(support_set_path):
    raise FileNotFoundError(
        f"Kh√¥ng t√¨m th·∫•y file {support_set_path}. "
        "Vui l√≤ng ƒë·∫£m b·∫£o file CSV ƒë√£ ƒë∆∞·ª£c t·∫°o v·ªõi c·∫•u tr√∫c: review_text, tag_label"
    )

# ƒê·ªçc CSV
df = pd.read_csv(support_set_path)

# Ki·ªÉm tra c·∫•u tr√∫c file
required_columns = ['review_text', 'tag_label']
if not all(col in df.columns for col in required_columns):
    raise ValueError(
        f"File CSV ph·∫£i c√≥ c√°c c·ªôt: {required_columns}. "
        f"C√°c c·ªôt hi·ªán t·∫°i: {list(df.columns)}"
    )

print(f"‚úÖ ƒê√£ ƒë·ªçc {len(df)} reviews t·ª´ {support_set_path}")
print(f"üìä S·ªë l∆∞·ª£ng tags duy nh·∫•t: {df['tag_label'].nunique()}")
print(f"üìã C√°c tags: {sorted(df['tag_label'].unique())}")

# Chuy·ªÉn ƒë·ªïi DataFrame th√†nh ƒë·ªëi t∆∞·ª£ng Dataset c·ªßa Hugging Face
raw_datasets = Dataset.from_pandas(df)

# ----------------------------------------------------------------------
# 2. KH·ªûI T·∫†O M√î H√åNH V√Ä TOKENIZER
# ----------------------------------------------------------------------

# T·∫°o mapping t·ª´ t√™n tag (string) sang ID s·ªë (integer)
unique_labels = sorted(list(set(df['tag_label'])))
label_to_id = {label: i for i, label in enumerate(unique_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}
NUM_TAGS = len(unique_labels)
print(f"Tags ƒëang ƒë∆∞·ª£c hu·∫•n luy·ªán: {unique_labels}. S·ªë l∆∞·ª£ng: {NUM_TAGS}")

MODEL_NAME = "vinai/phobert-base"
# S·ª≠ d·ª•ng 'use_fast=False' l√† b·∫Øt bu·ªôc ƒë·ªëi v·ªõi PhoBERT Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False) 

# T·∫£i m√¥ h√¨nh c∆° s·ªü v√† c·∫•u h√¨nh l·∫°i ƒë·∫ßu ph√¢n lo·∫°i (num_labels)
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=NUM_TAGS,
    id2label=id_to_label,
    label2id=label_to_id
)
print("ƒê√£ t·∫£i m√¥ h√¨nh PhoBERT base.")

# ----------------------------------------------------------------------
# 3. TI·ªÄN X·ª¨ L√ù V√Ä CHIA T·∫¨P D·ªÆ LI·ªÜU
# ----------------------------------------------------------------------

def tokenize_and_encode(examples):
    """Tokenize vƒÉn b·∫£n v√† chuy·ªÉn nh√£n (tag string) th√†nh ID s·ªë."""
    # max_length v√† padding r·∫•t quan tr·ªçng, ƒë·∫£m b·∫£o input ƒë·ªìng nh·∫•t
    tokenized = tokenizer(examples['review_text'], truncation=True, padding='max_length', max_length=128)
    # √Ånh x·∫° nh√£n string sang nh√£n s·ªë (ID)
    tokenized['labels'] = [label_to_id[label] for label in examples['tag_label']]
    return tokenized

# √Åp d·ª•ng Tokenizer cho to√†n b·ªô dataset
tokenized_few_shot = raw_datasets.map(tokenize_and_encode, batched=True)

# Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt cho training
# L∆∞u √Ω: '__index_level_0__' ch·ªâ c√≥ khi t·∫°o t·ª´ pandas DataFrame, c√≥ th·ªÉ kh√¥ng c√≥ khi ƒë·ªçc t·ª´ CSV
columns_to_remove = ['review_text', 'tag_label']
if '__index_level_0__' in tokenized_few_shot.column_names:
    columns_to_remove.append('__index_level_0__')
tokenized_few_shot = tokenized_few_shot.remove_columns(columns_to_remove)

# Chia t·∫≠p d·ªØ li·ªáu Few-Shot th√†nh train v√† test (validation)
split_dataset = tokenized_few_shot.train_test_split(test_size=0.2, seed=42) 

# ----------------------------------------------------------------------
# 4. C·∫§U H√åNH V√Ä CH·∫†Y TRAINER (FEW-SHOT SFT)
# ----------------------------------------------------------------------

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    """T√≠nh to√°n ƒë·ªô ch√≠nh x√°c (accuracy) trong qu√° tr√¨nh ƒë√°nh gi√°."""
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Tham s·ªë Hu·∫•n luy·ªán Few-Shot: CH·ªà C·∫¶N THAY ƒê·ªîI √çT, C√ì T√çNH M·ª§C TI√äU
training_args = TrainingArguments(
    output_dir="./phobert_few_shot_tags_classifier",
    learning_rate=1e-5, # T·ªëc ƒë·ªô h·ªçc t·∫≠p R·∫§T NH·ªé (Gi√∫p tinh ch·ªânh nh·∫π, tr√°nh l√†m h·ªèng ki·∫øn th·ª©c g·ªëc)
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3, # S·ªê EPOCH TH·∫§P (QUAN TR·ªåNG cho Few-Shot, tr√°nh overfitting v√†o d·ªØ li·ªáu nh·ªè)
    weight_decay=0.01,
    eval_strategy="epoch", # ƒê√°nh gi√° sau m·ªói epoch (ƒë·ªïi t√™n t·ª´ evaluation_strategy trong transformers m·ªõi)
    save_strategy="epoch", # L∆∞u model sau m·ªói epoch (ph·∫£i kh·ªõp v·ªõi eval_strategy khi d√πng load_best_model_at_end)
    load_best_model_at_end=True,
    logging_dir='./logs',
    dataloader_pin_memory=False, # T·∫Øt pin_memory ƒë·ªÉ tr√°nh c·∫£nh b√°o tr√™n MPS (Apple Silicon GPU)
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=split_dataset["train"],
    eval_dataset=split_dataset["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

print("\nB·∫Øt ƒë·∫ßu tinh ch·ªânh Few-Shot (PhoBERT)...")
trainer.train()

# ----------------------------------------------------------------------
# 5. L∆ØU V√Ä S·ª¨ D·ª§NG
# ----------------------------------------------------------------------

# L∆∞u m√¥ h√¨nh ƒë√£ fine-tuned v√† tokenizer v√†o th∆∞ m·ª•c Microservice AI c·ªßa b·∫°n
output_dir = "./final_few_shot_phobert_model"
os.makedirs(output_dir, exist_ok=True)

trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"\n‚úÖ Ho√†n t·∫•t. M√¥ h√¨nh v√† tokenizer ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_dir}")