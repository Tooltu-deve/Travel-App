"""
Script ƒë·ªÉ t√¨m ki·∫øm POI b·∫±ng Google Text Search API v√† scrape reviews b·∫±ng Selenium
- T√¨m POI b·∫±ng Google Places API (Text Search)
- L·ªçc POI c√≥ s·ªë l∆∞·ª£ng reviews > 100
- Scrape reviews t·ª´ Google Maps b·∫±ng Selenium
- Xu·∫•t ra file reviews.csv v·ªõi c√°c c·ªôt: placeID, reviews
"""

import os
import time
import csv
import json
import requests
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service

# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# Google Places API Key
GOOGLE_PLACES_API_KEY = os.getenv('GOOGLE_PLACES_API_KEY', '')

if not GOOGLE_PLACES_API_KEY:
    print("‚ö†Ô∏è  C·∫¢NH B√ÅO: GOOGLE_PLACES_API_KEY ch∆∞a ƒë∆∞·ª£c set trong bi·∫øn m√¥i tr∆∞·ªùng!")
    print("   H√£y set: export GOOGLE_PLACES_API_KEY='your_key'")
    exit(1)
else:
    print(f"‚úÖ GOOGLE_PLACES_API_KEY ƒë√£ ƒë∆∞·ª£c set (ƒë·ªô d√†i: {len(GOOGLE_PLACES_API_KEY)} k√Ω t·ª±)")

def search_pois_by_text(query: str, location: str = None, max_results: int = 100):
    """
    T√¨m ki·∫øm POI b·∫±ng Google Places API (Text Search) v·ªõi pagination
    
    Args:
        query: T·ª´ kh√≥a t√¨m ki·∫øm (v√≠ d·ª•: "restaurants in Ho Chi Minh City")
        location: V·ªã tr√≠ t√¨m ki·∫øm (optional, format: "lat,lng")
        max_results: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ t·ªëi ƒëa mu·ªën l·∫•y (m·∫∑c ƒë·ªãnh 100)
    
    Returns:
        List c√°c POI v·ªõi place_id, name, user_rating_total
    """
    url = "https://places.googleapis.com/v1/places:searchText"
    
    headers = {
        'Content-Type': 'application/json',
        'X-Goog-Api-Key': GOOGLE_PLACES_API_KEY,
        'X-Goog-FieldMask': 'places.id,places.displayName,places.userRatingCount,nextPageToken'
    }
    
    # T·∫°o request body
    body = {
        "textQuery": query,
        "maxResultCount": 20  # T·ªëi ƒëa 20 k·∫øt qu·∫£ m·ªói request
    }
    
    if location:
        # Parse location n·∫øu c√≥
        try:
            lat, lng = map(float, location.split(','))
            body["locationBias"] = {
                "circle": {
                    "center": {
                        "latitude": lat,
                        "longitude": lng
                    },
                    "radius": 5000.0  # 5km radius
                }
            }
        except:
            pass
    
    all_pois = []
    next_page_token = None
    page_count = 0
    
    try:
        while True:
            page_count += 1
            print(f"\nüìÑ ƒêang l·∫•y trang {page_count}...")
            
            # N·∫øu c√≥ nextPageToken t·ª´ l·∫ßn tr∆∞·ªõc, th√™m v√†o body
            if next_page_token:
                body["pageToken"] = next_page_token
            
            response = requests.post(url, headers=headers, json=body, timeout=10)
            
            if response.status_code != 200:
                print(f"‚ö†Ô∏è  API Error: HTTP {response.status_code}")
                print(f"   Response: {response.text[:200]}")
                break
            
            data = response.json()
            places = data.get('places', [])
            next_page_token = data.get('nextPageToken')
            
            print(f"  ‚Üí Nh·∫≠n ƒë∆∞·ª£c {len(places)} POI t·ª´ trang {page_count}")
            
            # X·ª≠ l√Ω t·ª´ng place
            for place in places:
                place_id = place.get('id', '')
                name = place.get('displayName', {}).get('text', '') if isinstance(place.get('displayName'), dict) else place.get('displayName', '')
                user_rating_count = place.get('userRatingCount', 0)
                
                # Ch·ªâ l·∫•y POI c√≥ s·ªë l∆∞·ª£ng reviews > 100
                if user_rating_count and user_rating_count > 100:
                    all_pois.append({
                        'place_id': place_id,
                        'name': name,
                        'user_rating_total': user_rating_count
                    })
                    print(f"    ‚úÖ {name}: {user_rating_count} reviews")
                else:
                    print(f"    ‚è≠Ô∏è  {name}: {user_rating_count} reviews (b·ªè qua, < 100)")
            
            # Ki·ªÉm tra ƒëi·ªÅu ki·ªán d·ª´ng
            if not next_page_token:
                print(f"  ‚Üí Kh√¥ng c√≤n trang ti·∫øp theo")
                break
            
            if len(all_pois) >= max_results:
                print(f"  ‚Üí ƒê√£ ƒë·∫°t gi·ªõi h·∫°n {max_results} POI")
                break
            
            # ƒê·ª£i m·ªôt ch√∫t tr∆∞·ªõc khi g·ªçi request ti·∫øp theo (tr√°nh rate limit)
            print(f"  ‚è≥ ƒê·ª£i 2 gi√¢y tr∆∞·ªõc khi l·∫•y trang ti·∫øp theo...")
            time.sleep(2)
            
            # X√≥a pageToken kh·ªèi body ƒë·ªÉ tr√°nh l·ªói n·∫øu kh√¥ng c√≥ nextPageToken
            if 'pageToken' in body:
                del body['pageToken']
        
        print(f"\n‚úÖ T·ªïng c·ªông l·∫•y ƒë∆∞·ª£c {len(all_pois)} POI ph√π h·ª£p t·ª´ {page_count} trang")
        return all_pois[:max_results]  # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng k·∫øt qu·∫£
        
    except Exception as e:
        print(f"‚ùå L·ªói khi g·ªçi API: {e}")
        import traceback
        traceback.print_exc()
        return all_pois  # Tr·∫£ v·ªÅ nh·ªØng g√¨ ƒë√£ l·∫•y ƒë∆∞·ª£c

def setup_selenium_driver():
    """
    Thi·∫øt l·∫≠p Selenium WebDriver
    """
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # Ch·∫°y ·ªü ch·∫ø ƒë·ªô headless (kh√¥ng hi·ªÉn th·ªã browser)
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    
    try:
        driver = webdriver.Chrome(options=chrome_options)
        return driver
    except Exception as e:
        print(f"‚ùå L·ªói khi kh·ªüi t·∫°o Selenium WebDriver: {e}")
        print("   G·ª£i √Ω: C√†i ƒë·∫∑t ChromeDriver v√† ƒë·∫£m b·∫£o n√≥ c√≥ trong PATH")
        return None

def scrape_reviews_from_google_maps(place_id: str, driver, max_reviews: int = 50):
    """
    Scrape reviews t·ª´ Google Maps b·∫±ng Selenium
    
    Args:
        place_id: Place ID c·ªßa POI
        driver: Selenium WebDriver instance
        max_reviews: S·ªë l∆∞·ª£ng reviews t·ªëi ƒëa c·∫ßn l·∫•y
    
    Returns:
        List c√°c review text
    """
    # URL Google Maps cho place_id - s·ª≠ d·ª•ng format ch√≠nh x√°c
    # C√°ch 1: S·ª≠ d·ª•ng place_id tr·ª±c ti·∫øp
    url = f"https://www.google.com/maps/place/?q=place_id:{place_id}"
    
    # C√°ch 2: N·∫øu c√°ch 1 kh√¥ng ho·∫°t ƒë·ªông, c√≥ th·ªÉ d√πng Places API ƒë·ªÉ l·∫•y t√™n v√† t√¨m ki·∫øm
    # Nh∆∞ng t·∫°m th·ªùi d√πng c√°ch 1
    
    reviews = []
    
    try:
        print(f"    ƒêang m·ªü Google Maps cho place_id: {place_id}...")
        driver.get(url)
        
        # ƒê·ª£i trang load
        wait = WebDriverWait(driver, 10)
        time.sleep(3)
        
        try:
            # T√¨m v√† click v√†o button "Reviews" ho·∫∑c scroll xu·ªëng ph·∫ßn reviews
            # Google Maps th∆∞·ªùng c√≥ button "Reviews" ho·∫∑c ph·∫ßn reviews ·ªü d∆∞·ªõi
            scroll_pause_time = 1.5
            screen_height = driver.execute_script("return window.innerHeight")
            
            # Scroll nhi·ªÅu l·∫ßn ƒë·ªÉ load reviews
            for i in range(5):
                driver.execute_script(f"window.scrollTo(0, {screen_height * (i + 1)});")
                time.sleep(scroll_pause_time)
            
            # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ reviews load
            time.sleep(2)
            
            # T√¨m c√°c element ch·ª©a reviews
            # Google Maps s·ª≠ d·ª•ng class ƒë·ªông, n√™n c·∫ßn th·ª≠ nhi·ªÅu selector
            review_texts = set()  # D√πng set ƒë·ªÉ tr√°nh duplicate
            
            # C√°c selector ph·ªï bi·∫øn cho review text trong Google Maps
            selectors = [
                "span.wiI7pd",  # Review text ch√≠nh
                "div.MyEned span",  # Review text trong container
                "div.jftiEf span.wiI7pd",  # Review text trong review card
                "div[data-review-id] span",  # Review text trong data-review-id
            ]
            
            for selector in selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    for elem in elements:
                        text = elem.text.strip()
                        # L·ªçc text h·ª£p l·ªá (ƒë·ªß d√†i, kh√¥ng ph·∫£i s·ªë, kh√¥ng ph·∫£i icon text)
                        if (text and 
                            len(text) > 20 and 
                            not text.isdigit() and
                            not text.startswith('‚òÖ') and
                            ':' not in text[:10]):  # B·ªè qua label nh∆∞ "5 stars:"
                            review_texts.add(text)
                except:
                    continue
            
            # N·∫øu v·∫´n ch∆∞a c√≥ reviews, th·ª≠ c√°ch kh√°c: t√¨m theo XPath
            if not review_texts:
                try:
                    # T√¨m t·∫•t c·∫£ div c√≥ ch·ª©a text d√†i (c√≥ th·ªÉ l√† reviews)
                    all_divs = driver.find_elements(By.XPATH, "//div[contains(@class, 'MyEned') or contains(@class, 'jftiEf')]")
                    for div in all_divs:
                        text = div.text.strip()
                        # L·ªçc text h·ª£p l·ªá
                        if (text and 
                            len(text) > 30 and 
                            len(text) < 2000 and  # Reviews th∆∞·ªùng kh√¥ng qu√° d√†i
                            '\n' in text and  # Reviews th∆∞·ªùng c√≥ nhi·ªÅu d√≤ng
                            not text.startswith('‚òÖ')):
                            # L·∫•y d√≤ng ƒë·∫ßu ti√™n ho·∫∑c to√†n b·ªô text
                            lines = text.split('\n')
                            for line in lines:
                                if len(line) > 20:
                                    review_texts.add(line)
                except:
                    pass
            
            # Chuy·ªÉn set th√†nh list v√† gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
            reviews = list(review_texts)[:max_reviews]
            
        except TimeoutException:
            print(f"    ‚ö†Ô∏è  Timeout khi ƒë·ª£i reviews load")
        except NoSuchElementException:
            print(f"    ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y ph·∫ßn reviews")
        except Exception as e:
            print(f"    ‚ö†Ô∏è  L·ªói khi scrape reviews: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"    ‚úÖ L·∫•y ƒë∆∞·ª£c {len(reviews)} reviews")
        
    except Exception as e:
        print(f"    ‚ùå L·ªói khi scrape reviews cho {place_id}: {e}")
        import traceback
        traceback.print_exc()
    
    return reviews

# Danh s√°ch 20 th√†nh ph·ªë n·ªïi ti·∫øng nh·∫•t ·ªü Vi·ªát Nam v·ªõi t·ªça ƒë·ªô trung t√¢m
VIETNAM_CITIES = [
    {"name": "H√† N·ªôi", "lat": 21.0285, "lng": 105.8542},
    {"name": "Th√†nh ph·ªë H·ªì Ch√≠ Minh", "lat": 10.8231, "lng": 106.6297},
    {"name": "ƒê√† N·∫µng", "lat": 16.0544, "lng": 108.2022},
    {"name": "H·∫£i Ph√≤ng", "lat": 20.8449, "lng": 106.6881},
    {"name": "C·∫ßn Th∆°", "lat": 10.0452, "lng": 105.7469},
    {"name": "Nha Trang", "lat": 12.2388, "lng": 109.1967},
    {"name": "Hu·∫ø", "lat": 16.4637, "lng": 107.5909},
    {"name": "V≈©ng T√†u", "lat": 10.3460, "lng": 107.0843},
    {"name": "Phan Thi·∫øt", "lat": 10.9376, "lng": 108.1018},
    {"name": "Quy Nhon", "lat": 13.7765, "lng": 109.2237},
    {"name": "H·∫° Long", "lat": 20.9101, "lng": 107.1839},
    {"name": "Sapa", "lat": 22.3364, "lng": 103.8437},
    {"name": "ƒê√† L·∫°t", "lat": 11.9404, "lng": 108.4583},
    {"name": "H·ªôi An", "lat": 15.8801, "lng": 108.3380},
    {"name": "Ph√∫ Qu·ªëc", "lat": 10.2899, "lng": 103.9840},
    {"name": "M≈©i N√©", "lat": 10.9600, "lng": 108.2800},
    {"name": "Tam ƒê·∫£o", "lat": 21.4500, "lng": 105.6500},
    {"name": "C√°t B√†", "lat": 20.8000, "lng": 107.0167},
    {"name": "Mai Ch√¢u", "lat": 20.6667, "lng": 105.0833},
    {"name": "M·ªôc Ch√¢u", "lat": 20.8500, "lng": 104.6333},
]

def main():
    """
    H√†m ch√≠nh ƒë·ªÉ t√¨m POI v√† scrape reviews cho 20 th√†nh ph·ªë n·ªïi ti·∫øng nh·∫•t ·ªü Vi·ªát Nam
    """
    print("\n" + "="*60)
    print("SCRAPER POI REVIEWS - Google Places API + Selenium")
    print("T·ª± ƒë·ªông ch·∫°y cho 20 th√†nh ph·ªë n·ªïi ti·∫øng nh·∫•t ·ªü Vi·ªát Nam")
    print("="*60)
    
    # H·ªèi s·ªë l∆∞·ª£ng POI t·ªëi ƒëa cho m·ªói th√†nh ph·ªë
    max_results_input = input("\nS·ªë l∆∞·ª£ng POI t·ªëi ƒëa cho m·ªói th√†nh ph·ªë (m·∫∑c ƒë·ªãnh 50, nh·∫•n Enter ƒë·ªÉ d√πng m·∫∑c ƒë·ªãnh): ").strip()
    max_results_per_city = int(max_results_input) if max_results_input.isdigit() else 50
    
    # H·ªèi c√≥ mu·ªën scrape reviews kh√¥ng
    scrape_reviews = input("\nB·∫°n c√≥ mu·ªën scrape reviews t·ª´ Google Maps kh√¥ng? (y/n, m·∫∑c ƒë·ªãnh: n): ").strip().lower()
    scrape_reviews = scrape_reviews == 'y'
    
    # T·∫°o th∆∞ m·ª•c reviews n·∫øu ch∆∞a c√≥
    os.makedirs('./reviews', exist_ok=True)
    
    # Thi·∫øt l·∫≠p Selenium n·∫øu c·∫ßn scrape reviews
    driver = None
    if scrape_reviews:
        print("\nüöÄ ƒêang kh·ªüi t·∫°o Selenium WebDriver...")
        driver = setup_selenium_driver()
        if not driver:
            print("‚ö†Ô∏è  Kh√¥ng th·ªÉ kh·ªüi t·∫°o Selenium. Ch·ªâ t√¨m ki·∫øm POI, kh√¥ng scrape reviews.")
            scrape_reviews = False
    
    # T·ªïng h·ª£p d·ªØ li·ªáu t·ª´ t·∫•t c·∫£ th√†nh ph·ªë
    all_reviews_data = []
    all_pois_summary = []
    
    # Ch·∫°y cho t·ª´ng th√†nh ph·ªë
    for city_idx, city in enumerate(VIETNAM_CITIES, 1):
        print("\n" + "="*60)
        print(f"[{city_idx}/{len(VIETNAM_CITIES)}] ƒêang x·ª≠ l√Ω: {city['name']}")
        print("="*60)
        
        # T·∫°o query
        query = f"ƒê·ªãa ƒëi·ªÉm du l·ªãch v√† th·∫Øng c·∫£nh ·ªü {city['name']}"
        location = f"{city['lat']},{city['lng']}"
        
        print(f"üîç Query: {query}")
        print(f"üìç Location: {city['name']} ({city['lat']}, {city['lng']})")
        print(f"   Gi·ªõi h·∫°n: {max_results_per_city} POI")
        
        try:
            # T√¨m ki·∫øm POI v·ªõi pagination
            pois = search_pois_by_text(query, location, max_results=max_results_per_city)
            
            if not pois:
                print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y POI n√†o ph√π h·ª£p cho {city['name']}")
                continue
            
            print(f"\n‚úÖ T√¨m th·∫•y {len(pois)} POI c√≥ > 100 reviews cho {city['name']}")
            
            # L∆∞u summary POI
            for poi in pois:
                all_pois_summary.append({
                    'city': city['name'],
                    'place_id': poi['place_id'],
                    'name': poi['name'],
                    'user_rating_total': poi['user_rating_total']
                })
            
            # Scrape reviews n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
            if scrape_reviews and driver:
                print(f"\nüìù ƒêang scrape reviews cho {len(pois)} POI ·ªü {city['name']}...")
                
                for idx, poi in enumerate(pois, 1):
                    print(f"\n  [{idx}/{len(pois)}] {poi['name']}")
                    print(f"      Place ID: {poi['place_id']}")
                    print(f"      S·ªë reviews: {poi['user_rating_total']}")
                    
                    reviews = scrape_reviews_from_google_maps(poi['place_id'], driver, max_reviews=50)
                    
                    if reviews:
                        # L∆∞u t·ª´ng review nh∆∞ m·ªôt d√≤ng ri√™ng
                        for review in reviews:
                            all_reviews_data.append({
                                'placeID': poi['place_id'],
                                'reviews': review
                            })
                    
                    # Ngh·ªâ gi·ªØa c√°c request ƒë·ªÉ tr√°nh b·ªã block
                    if idx < len(pois):
                        time.sleep(2)
            
        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω {city['name']}: {e}")
            import traceback
            traceback.print_exc()
            continue  # Ti·∫øp t·ª•c v·ªõi th√†nh ph·ªë ti·∫øp theo
        
        # Ngh·ªâ gi·ªØa c√°c th√†nh ph·ªë
        if city_idx < len(VIETNAM_CITIES):
            print(f"\n‚è≥ ƒê·ª£i 3 gi√¢y tr∆∞·ªõc khi chuy·ªÉn sang th√†nh ph·ªë ti·∫øp theo...")
            time.sleep(3)
    
    # ƒê√≥ng browser n·∫øu c√≥
    if driver:
        driver.quit()
    
    # L∆∞u summary POI
    summary_file = './reviews/pois_summary.csv'
    print(f"\nüíæ ƒêang l∆∞u summary POI v√†o {summary_file}...")
    try:
        with open(summary_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['city', 'place_id', 'name', 'user_rating_total'])
            writer.writeheader()
            writer.writerows(all_pois_summary)
        print(f"‚úÖ ƒê√£ l∆∞u {len(all_pois_summary)} POI v√†o {summary_file}")
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u summary: {e}")
    
    # L∆∞u reviews v√†o CSV (n·∫øu c√≥)
    if all_reviews_data:
        output_file = './reviews/all_reviews.csv'
        print(f"\nüíæ ƒêang l∆∞u {len(all_reviews_data)} reviews v√†o {output_file}...")
        
        try:
            # Ki·ªÉm tra file ƒë√£ t·ªìn t·∫°i ch∆∞a ƒë·ªÉ append ho·∫∑c t·∫°o m·ªõi
            file_exists = os.path.exists(output_file)
            with open(output_file, 'a' if file_exists else 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=['placeID', 'reviews'])
                if not file_exists:
                    writer.writeheader()
                writer.writerows(all_reviews_data)
            
            print(f"‚úÖ ƒê√£ l∆∞u th√†nh c√¥ng v√†o {output_file}")
            print(f"   - T·ªïng s·ªë reviews: {len(all_reviews_data)}")
            
            # Th·ªëng k√™ theo th√†nh ph·ªë
            city_stats = {}
            for poi in all_pois_summary:
                city = poi['city']
                if city not in city_stats:
                    city_stats[city] = {'pois': 0, 'reviews': 0}
                city_stats[city]['pois'] += 1
            
            # ƒê·∫øm reviews theo placeID v√† map v·ªÅ city
            poi_to_city = {poi['place_id']: poi['city'] for poi in all_pois_summary}
            for row in all_reviews_data:
                place_id = row['placeID']
                if place_id in poi_to_city:
                    city = poi_to_city[place_id]
                    city_stats[city]['reviews'] += 1
            
            print(f"\nüìä Th·ªëng k√™ theo th√†nh ph·ªë:")
            for city, stats in sorted(city_stats.items()):
                print(f"   - {city}: {stats['pois']} POI, {stats['reviews']} reviews")
            
        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u file CSV: {e}")
    else:
        print("\n‚ö†Ô∏è  Kh√¥ng c√≥ reviews n√†o ƒë∆∞·ª£c scrape (c√≥ th·ªÉ do kh√¥ng ch·ªçn scrape ho·∫∑c l·ªói)")
    
    # T·ªïng k·∫øt
    print("\n" + "="*60)
    print("‚úÖ HO√ÄN T·∫§T!")
    print("="*60)
    print(f"   - ƒê√£ x·ª≠ l√Ω: {len(VIETNAM_CITIES)} th√†nh ph·ªë")
    print(f"   - T·ªïng s·ªë POI: {len(all_pois_summary)}")
    if all_reviews_data:
        print(f"   - T·ªïng s·ªë reviews: {len(all_reviews_data)}")
    print(f"   - File summary: {summary_file}")
    if all_reviews_data:
        print(f"   - File reviews: ./reviews/all_reviews.csv")

if __name__ == "__main__":
    main()

